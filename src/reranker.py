import openai
from typing import List, Dict, Any
from logging import log
from configs import config
from src.prompt_engineering import format_rerank_prompt


class Reranker:
    """
    A class for reranking evidence documents based on LLM-generated relevance scores.
    """

    def __init__(self):
        """
        Initializes the EvidenceReranker with OpenAI API and model configurations.
        """
        self.openai_client = openai.OpenAI(api_key=config.open_api_key)
        self.model = config.gpt_model
        self.max_results = config.top_rerank

        # Load weights from config
        self.weight_vector = getattr(config, "rerank_weight_vector", 0.35)  
        self.weight_llm = getattr(config, "rerank_weight_llm", 0.65)  

    def rank_evidence(self, query: str, evidence_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Rerank retrieved evidence based on relevance scores generated by the LLM.

        Args:
            query (str): The investigator's search query.
            evidence_list (List[Dict[str, Any]]): A list of evidence documents.

        Returns:
            List[Dict[str, Any]]: A list of reranked evidence, sorted by relevance.
        """
        if not evidence_list:
            log.warning("No evidence found for reranking.")
            return []

        batch_size = 3
        ranked_evidence = []

        for start in range(0, len(evidence_list), batch_size):
            batch = evidence_list[start: start + batch_size]
            relevance_scores = self._evaluate_relevance(query, batch)

            for idx, evidence in enumerate(batch):
                vector_weight = evidence.get("vector_score", 0.5)
                llm_weight = relevance_scores[idx]
                combined = self._compute_final_score(vector_weight, llm_weight)

                ranked_evidence.append({
                    "id": evidence["id"],
                    "text": evidence["text"],
                    "metadata": evidence.get("metadata", {}),
                    "vector_score": vector_weight,
                    "llm_score": llm_weight,
                    "final_score": combined,
                    "confidence_label": self._categorize_confidence(combined),
                })

        ranked_evidence.sort(key=lambda x: x["final_score"], reverse=True)
        return ranked_evidence[:self.max_results]

    def _categorize_confidence(self, score: float) -> str:
        """
        Assigns a confidence label based on the final computed score.

        Args:
            score (float): The final reranking score.

        Returns:
            str: Confidence label (e.g., "High", "Moderate", etc.).
        """
        if score >= 0.8:
            return "Extremely High"
        elif score >= 0.6:
            return "High"
        elif score >= 0.4:
            return "Moderate"
        elif score >= 0.2:
            return "Low"
        else:
            return "Very Low"

    def _evaluate_relevance(self, query: str, documents: List[Dict[str, Any]]) -> List[float]:
        """
        Calls the LLM to evaluate document relevance on a scale of 1-10, then normalizes scores.

        Args:
            query (str): The search query.
            documents (List[Dict[str, Any]]): A batch of documents to be scored.

        Returns:
            List[float]: A list of normalized scores between 0.2 and 1.0.
        """
        scores = []
        for doc in documents:
            try:
                response = self.openai_client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are an expert cybercrime investigator."},
                        {"role": "user", "content": format_rerank_prompt(query, doc["text"])}
                    ],
                    temperature=0.2,
                    max_tokens=5
                )

                score_text = response.choices[0].message.content.strip()
                numeric_score = int(''.join(filter(str.isdigit, score_text)))
                
                normalized_score = round(max(2, min(10, numeric_score)) / 10.0, 1)

                scores.append(normalized_score)

            except Exception as e:
                log.error(f"Error reranking document ID {doc['id']}: {e}")
                scores.append(0.4)
        return scores


    def _compute_final_score(self, vector_score: float, llm_score: float) -> float:
        """
        Computes the final reranking score using a weighted combination.

        Args:
            vector_score (float): The precomputed vector similarity score.
            llm_score (float): The relevance score assigned by the LLM.

        Returns:
            float: The combined score weighted between vector and LLM results.
        """
        return (vector_score * self.weight_vector) + (llm_score * self.weight_llm)
